{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"qY86fWBFvWUG","executionInfo":{"status":"error","timestamp":1713687617456,"user_tz":-120,"elapsed":17,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}},"outputId":"86dbed07-36ed-4a88-82a6-10d67c4c1937"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e117ad73df99>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import numpy as np\n","import tempfile\n","from datasets import load_dataset\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3FaCYwDvWUK","executionInfo":{"status":"aborted","timestamp":1713687617457,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset('KaungHtetCho/MedicalQA')\n","seed = 123\n","\n","# Split the dataset into training and test data (90% train, 10% test)\n","reduce_dataset_split = dataset[\"train\"].train_test_split(test_size=0.001, seed=seed)\n","train_set = reduce_dataset_split['test']\n","\n","train_test_split = train_set.train_test_split(test_size=0.1, seed=seed)\n","test_set = train_test_split['test']\n","\n","# Further split the test set into validation and test sets (50% validation, 50% test)\n","val_test_split = test_set.train_test_split(test_size=0.5, seed=seed)\n","validation_set = val_test_split['train']\n","test_set = val_test_split['test']\n","\n","# Update dataset to include these splits\n","dataset[\"train\"] = train_test_split['train']\n","dataset[\"validation\"] = validation_set\n","dataset[\"test\"] = test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ny3hEAMvWUL","executionInfo":{"status":"aborted","timestamp":1713687617458,"user_tz":-120,"elapsed":12,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["def concatenate_utterances(example):\n","    example['dialog'] = example['Patient'] + \" \" + example['Doctor']\n","    del example['Description']\n","    del example['Patient']\n","    del example['Doctor']\n","    return example\n","\n","dataset = dataset.map(concatenate_utterances, remove_columns=['Description', 'Patient', 'Doctor'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfCXDbYDvWUM","executionInfo":{"status":"aborted","timestamp":1713687617458,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98IJ6O2GvWUM","executionInfo":{"status":"aborted","timestamp":1713687617458,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["# Load the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')\n","tokenizer.pad_token = tokenizer.eos_token\n","model = GPT2LMHeadModel.from_pretrained('microsoft/DialoGPT-small')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ET2qCPgKvWUM","executionInfo":{"status":"aborted","timestamp":1713687617458,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["# Encode the dataset\n","def encode(examples):\n","    encoded = tokenizer(examples['dialog'], truncation=True, padding='max_length', max_length=128)\n","    encoded['labels'] = encoded['input_ids'][:]\n","    return encoded\n","\n","encoded_dataset = dataset.map(encode, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mElMcY2avWUN","executionInfo":{"status":"aborted","timestamp":1713687617459,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=tempfile.mkdtemp(),   # output directory\n","    num_train_epochs=1,             # total number of training epochs\n","    per_device_train_batch_size=2,  # batch size per device during training\n","    per_device_eval_batch_size=2,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir=None,                # directory for storing logs\n","    fp16=True                        # use floating point 16 bit precision for training\n",")\n","\n","# Create Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_dataset['train'],\n","    eval_dataset=encoded_dataset['validation']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9vpLsyavWUN","executionInfo":{"status":"aborted","timestamp":1713687617459,"user_tz":-120,"elapsed":11,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["# Evaluate before fine-tuning\n","pre_eval_results = trainer.evaluate(encoded_dataset['validation'])\n","\n","# Get predictions for validation set before fine tuning for 10 samples\n","pre_val_predictions = trainer.predict(encoded_dataset['validation'].select(range(10)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbYxTbDzvWUO","executionInfo":{"status":"aborted","timestamp":1713687617460,"user_tz":-120,"elapsed":12,"user":{"displayName":"Myo Thiha","userId":"00141362449010417331"}}},"outputs":[],"source":["# Fine-tune the model\n","trainer.train()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}